{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get necessary Imports\n",
    "import pdb\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from albumentations import (Normalize, Compose)\n",
    "from albumentations.pytorch import ToTensor\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from mishefficientnet_hengs\timport *\n",
    "from misheffnet_b5utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DIR = '../input/hengs-split'\n",
    "DATA_DIR = '../input/severstal-steel-defect-detection'\n",
    "CHECKPOINT_FILE = '../input/more-train-mish-inference/00095000_model.pth'\n",
    "SUBMISSION_CSV_FILE = '../working/submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_submission_csv(df):\n",
    "\n",
    "\n",
    "    text = ''\n",
    "    df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "    df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "    num_image = len(df)//4\n",
    "    num = len(df)\n",
    "\n",
    "    pos = (df['Label']==1).sum()\n",
    "    neg = num-pos\n",
    "\n",
    "\n",
    "    pos1 = ((df['Class']==1) & (df['Label']==1)).sum()\n",
    "    pos2 = ((df['Class']==2) & (df['Label']==1)).sum()\n",
    "    pos3 = ((df['Class']==3) & (df['Label']==1)).sum()\n",
    "    pos4 = ((df['Class']==4) & (df['Label']==1)).sum()\n",
    "\n",
    "    neg1 = num_image-pos1\n",
    "    neg2 = num_image-pos2\n",
    "    neg3 = num_image-pos3\n",
    "    neg4 = num_image-pos4\n",
    "\n",
    "\n",
    "    text += 'compare with LB probing ... \\n'\n",
    "    text += '\\t\\tnum_image = %5d(1801) \\n'%num_image\n",
    "    text += '\\t\\tnum  = %5d(7204) \\n'%num\n",
    "    text += '\\n'\n",
    "\n",
    "    text += '\\t\\tpos1 = %5d( 128)  %0.3f\\n'%(pos1,pos1/128)\n",
    "    text += '\\t\\tpos2 = %5d(  43)  %0.3f\\n'%(pos2,pos2/43)\n",
    "    text += '\\t\\tpos3 = %5d( 741)  %0.3f\\n'%(pos3,pos3/741)\n",
    "    text += '\\t\\tpos4 = %5d( 120)  %0.3f\\n'%(pos4,pos4/120)\n",
    "    text += '\\n'\n",
    "\n",
    "    text += '\\t\\tneg1 = %5d(1673)  %0.3f  %3d\\n'%(neg1,neg1/1673, neg1-1673)\n",
    "    text += '\\t\\tneg2 = %5d(1758)  %0.3f  %3d\\n'%(neg2,neg2/1758, neg2-1758)\n",
    "    text += '\\t\\tneg3 = %5d(1060)  %0.3f  %3d\\n'%(neg3,neg3/1060, neg3-1060)\n",
    "    text += '\\t\\tneg4 = %5d(1681)  %0.3f  %3d\\n'%(neg4,neg4/1681, neg4-1681)\n",
    "    text += '--------------------------------------------------\\n'\n",
    "    text += '\\t\\tneg  = %5d(6172)  %0.3f  %3d \\n'%(neg,neg/6172, neg-6172)\n",
    "    text += '\\n'\n",
    "\n",
    "    if 1:\n",
    "        #compare with reference\n",
    "        pass\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGnUp2d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, num_group=32, kernel_size=3, padding=1, stride=1):\n",
    "        super(ConvGnUp2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        self.gn   = nn.GroupNorm(num_group,out_channel)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.gn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "def upsize_add(x, lateral):\n",
    "    return F.interpolate(x, size=lateral.shape[2:], mode='nearest') + lateral\n",
    "\n",
    "def upsize(x, scale_factor=2):\n",
    "    x = F.interpolate(x, scale_factor=scale_factor, mode='nearest')\n",
    "    return x\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def load_pretrain(self, skip=['logit.'], is_print=True):\n",
    "        load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=CONVERSION, is_print=is_print)\n",
    "\n",
    "    def __init__(self, num_class=4, drop_connect_rate=0.2):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        e = EfficientNetB5(drop_connect_rate)\n",
    "        self.stem   = e.stem\n",
    "        self.block1 = e.block1\n",
    "        self.block2 = e.block2\n",
    "        self.block3 = e.block3\n",
    "        self.block4 = e.block4\n",
    "        self.block5 = e.block5\n",
    "        self.block6 = e.block6\n",
    "        self.block7 = e.block7\n",
    "        self.last   = e.last\n",
    "        e = None  #dropped\n",
    "\n",
    "        #---\n",
    "        self.lateral0 = nn.Conv2d(2048, 64,  kernel_size=1, padding=0, stride=1)\n",
    "        self.lateral1 = nn.Conv2d( 176, 64,  kernel_size=1, padding=0, stride=1)\n",
    "        self.lateral2 = nn.Conv2d(  64, 64,  kernel_size=1, padding=0, stride=1)\n",
    "        self.lateral3 = nn.Conv2d(  40, 64,  kernel_size=1, padding=0, stride=1)\n",
    "\n",
    "        self.top1 = nn.Sequential(\n",
    "            ConvGnUp2d( 64, 64),\n",
    "            ConvGnUp2d( 64, 64),\n",
    "            ConvGnUp2d( 64, 64),\n",
    "        )\n",
    "        self.top2 = nn.Sequential(\n",
    "            ConvGnUp2d( 64, 64),\n",
    "            ConvGnUp2d( 64, 64),\n",
    "        )\n",
    "        self.top3 = nn.Sequential(\n",
    "            ConvGnUp2d( 64, 64),\n",
    "        )\n",
    "        self.top4 = nn.Sequential(\n",
    "            nn.Conv2d(64*3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.logit_mask = nn.Conv2d(64,num_class+1,kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "        # x = x.clone()\n",
    "        # x = x-torch.FloatTensor(IMAGE_RGB_MEAN).to(x.device).view(1,-1,1,1)\n",
    "        # x = x/torch.FloatTensor(IMAGE_RGB_STD).to(x.device).view(1,-1,1,1)\n",
    "\n",
    "        x = self.stem(x)            #; print('stem  ',x.shape)\n",
    "        x = self.block1(x)    ;x0=x #; print('block1',x.shape)\n",
    "        x = self.block2(x)    ;x1=x #; print('block2',x.shape)\n",
    "        x = self.block3(x)    ;x2=x #; print('block3',x.shape)\n",
    "        x = self.block4(x)          #; print('block4',x.shape)\n",
    "        x = self.block5(x)    ;x3=x #; print('block5',x.shape)\n",
    "        x = self.block6(x)          #; print('block6',x.shape)\n",
    "        x = self.block7(x)          #; print('block7',x.shape)\n",
    "        x = self.last(x)      ;x4=x #; print('last  ',x.shape)\n",
    "\n",
    "        # segment\n",
    "        t0 = self.lateral0(x4)\n",
    "        t1 = upsize_add(t0, self.lateral1(x3)) #16x16\n",
    "        t2 = upsize_add(t1, self.lateral2(x2)) #32x32\n",
    "        t3 = upsize_add(t2, self.lateral3(x1)) #64x64\n",
    "\n",
    "        t1 = self.top1(t1) #128x128\n",
    "        t2 = self.top2(t2) #128x128\n",
    "        t3 = self.top3(t3) #128x128\n",
    "\n",
    "        t = torch.cat([t1,t2,t3],1)\n",
    "        t = self.top4(t)\n",
    "        logit_mask = self.logit_mask(t)\n",
    "        logit_mask = F.interpolate(logit_mask, scale_factor=2.0, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return logit_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_input(image):\n",
    "    input = image.astype(np.float32)\n",
    "    input = input[...,::-1]/255\n",
    "    input = input.transpose(0,3,1,2)\n",
    "    # input[:,0] = (input[:,0]-IMAGE_RGB_MEAN[0])/IMAGE_RGB_STD[0]\n",
    "    # input[:,1] = (input[:,1]-IMAGE_RGB_MEAN[1])/IMAGE_RGB_STD[1]\n",
    "    # input[:,2] = (input[:,2]-IMAGE_RGB_MEAN[2])/IMAGE_RGB_STD[2]\n",
    "    return input\n",
    "\n",
    "\n",
    "class KaggleTestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        df =  pd.read_csv(DATA_DIR + '/sample_submission.csv')\n",
    "        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "        self.uid = df['ImageId'].unique().tolist()\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uid)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        image_id = self.uid[index]\n",
    "        image = cv2.imread(DATA_DIR + '/test_images/%s'%(image_id), cv2.IMREAD_COLOR)\n",
    "        return image, image_id\n",
    "\n",
    "\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    input = []\n",
    "    image_id = []\n",
    "    for b in range(batch_size):\n",
    "        input.append(batch[b][0])\n",
    "        image_id.append(batch[b][1])\n",
    "    input = np.stack(input)\n",
    "    input = torch.from_numpy(image_to_input(input))\n",
    "    return input, image_id\n",
    "\n",
    "def run_length_encode(mask):\n",
    "    #possible bug for here\n",
    "    m = mask.T.flatten()\n",
    "    if m.sum()==0:\n",
    "        rle=''\n",
    "    else:\n",
    "        m   = np.concatenate([[0], m, [0]])\n",
    "        run = np.where(m[1:] != m[:-1])[0] + 1\n",
    "        run[1::2] -= run[::2]\n",
    "        rle = ' '.join(str(r) for r in run)\n",
    "    return rle\n",
    "\n",
    "def probability_mask_to_probability_label(probability):\n",
    "    batch_size,num_class,H,W = probability.shape\n",
    "    probability = probability.permute(0, 2, 3, 1).contiguous().view(batch_size,-1, 5)\n",
    "    value, index = probability.max(1)\n",
    "    probability = value[:,1:]\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load net ...\n",
      "\n",
      "load data ...\n",
      "\tlen = 1801\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f06a085ea8041dda6bc513130db1956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loader: t =  400 /  450  torch.Size([4, 3, 256, 1600])  e4f7dd7de.jpg :  2 min 03 sec\n",
      "\n",
      "compare with LB probing ... \n",
      "\t\tnum_image =  1801(1801) \n",
      "\t\tnum  =  7204(7204) \n",
      "\n",
      "\t\tpos1 =   131( 128)  1.023\n",
      "\t\tpos2 =    58(  43)  1.349\n",
      "\t\tpos3 =   151( 741)  0.204\n",
      "\t\tpos4 =   111( 120)  0.925\n",
      "\n",
      "\t\tneg1 =  1670(1673)  0.998   -3\n",
      "\t\tneg2 =  1743(1758)  0.991  -15\n",
      "\t\tneg3 =  1650(1060)  1.557  590\n",
      "\t\tneg4 =  1690(1681)  1.005    9\n",
      "--------------------------------------------------\n",
      "\t\tneg  =  6753(6172)  1.094  581 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# threshold_label      = [ 0.50, 0.50, 0.50, 0.50,]\n",
    "# threshold_mask_pixel = [ 0.50, 0.50, 0.50, 0.50,]\n",
    "# threshold_mask_size  = [ 1,  1,  1,  1,]\n",
    "\n",
    "threshold_label      = [ 0.80, 0.90, 0.75, 0.60,]\n",
    "threshold_mask_pixel = [ 0.40, 0.40, 0.40, 0.40,]\n",
    "threshold_mask_size  = [   40,   40,   40,   40,]\n",
    "\n",
    "## load net\n",
    "print('load net ...')\n",
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(CHECKPOINT_FILE, map_location=lambda storage, loc: storage))\n",
    "print('')\n",
    "\n",
    "\n",
    "## load data\n",
    "print('load data ...')\n",
    "dataset = KaggleTestDataset()\n",
    "print(dataset)\n",
    "#exit(0)\n",
    "\n",
    "loader  = DataLoader(\n",
    "    dataset,\n",
    "    sampler     = SequentialSampler(dataset),\n",
    "    batch_size  = 4,\n",
    "    drop_last   = False,\n",
    "    num_workers = 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = null_collate\n",
    ")\n",
    "\n",
    "\n",
    "## start here ----------------------------------\n",
    "image_id_class_id = []\n",
    "encoded_pixel     = []\n",
    "\n",
    "\n",
    "net.eval()\n",
    "\n",
    "import tqdm\n",
    "start_timer = timer()\n",
    "for t,(input, image_id) in tqdm.tqdm_notebook(enumerate(loader)):\n",
    "    if t%200==0:\n",
    "        print('\\r loader: t = %4d / %4d  %s  %s : %s'%(\n",
    "              t, len(loader)-1, str(input.shape), image_id[0], time_to_str((timer() - start_timer),'sec'),\n",
    "        ),end='', flush=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input = input.cuda()\n",
    "\n",
    "        logit = net(input) #data_parallel(net,input)  #net(input)\n",
    "        probability = torch.softmax(logit,1)\n",
    "        \n",
    "        probability_mask  = probability[:,1:]#just drop background\n",
    "        probability_label = probability_mask_to_probability_label(probability[:,1:])\n",
    "\n",
    "    probability_mask  = probability_mask.data.cpu().numpy()\n",
    "    probability_label = probability_label.data.cpu().numpy()\n",
    "\n",
    "    batch_size = len(image_id)\n",
    "    for b in range(batch_size):\n",
    "        for c in range(4):\n",
    "            rle=''\n",
    "            predict_label = probability_label[b,c]>threshold_label[c]\n",
    "            if predict_label:\n",
    "                try:\n",
    "                    predict_mask = probability_mask[b,c] > threshold_mask_pixel[c]\n",
    "                    #predict_mask = post_process(predict_mask, threshold_mask_size[c])\n",
    "                    rle = run_length_encode(predict_mask)\n",
    "\n",
    "                except:\n",
    "                    print('An exception occurred : %s'%(image_id[b]+'_%d'%(c+1)))\n",
    "\n",
    "\n",
    "            image_id_class_id.append(image_id[b]+'_%d'%(c+1))\n",
    "            encoded_pixel.append(rle)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(zip(image_id_class_id, encoded_pixel), columns=['ImageId_ClassId', 'EncodedPixels'])\n",
    "df.to_csv(SUBMISSION_CSV_FILE, index=False)\n",
    "print('')\n",
    "\n",
    "## print statistics ----\n",
    "if 1:\n",
    "    text = summarise_submission_csv(df)\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1493e1a9f9924f398ce345455e44a5aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f06a085ea8041dda6bc513130db1956": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5c593ee521d24941bcf5c5eb36ddcd45",
        "IPY_MODEL_448c8eb1824d44e49a845ce269291bb4"
       ],
       "layout": "IPY_MODEL_d54a71abe3fa40fd88d427ef6e04e79e"
      }
     },
     "448c8eb1824d44e49a845ce269291bb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1493e1a9f9924f398ce345455e44a5aa",
       "placeholder": "​",
       "style": "IPY_MODEL_6dc909ab32074358a56567a56ad8503c",
       "value": " 451/? [02:19&lt;00:00,  3.24it/s]"
      }
     },
     "5c593ee521d24941bcf5c5eb36ddcd45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_833cf7a4a42c45de98608305b4b2a30e",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d5bfc796c66a4f0faaade3af719157eb",
       "value": 1
      }
     },
     "6dc909ab32074358a56567a56ad8503c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "833cf7a4a42c45de98608305b4b2a30e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d54a71abe3fa40fd88d427ef6e04e79e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5bfc796c66a4f0faaade3af719157eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
