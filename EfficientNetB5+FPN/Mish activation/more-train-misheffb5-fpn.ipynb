{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/effb5-mishfpn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../usr/lib","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\n\nimport random \nfrom timeit import default_timer as timer\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, Sampler\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch\n\nfrom mishefficientnet_hengs import EfficientNetB5\nfrom mishefficientnet_hengs import CONVERSION\nfrom misheffnet_b5utility  import *\n# from heng_s_utility_functions import *\n\nPI = np.pi\nIMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\nIMAGE_RGB_STD  = [0.229, 0.224, 0.225]\nDEFECT_COLOR = [(0,0,0),(0,0,255),(0,255,0),(255,0,0),(0,255,255)]\nSEED = 69","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def seed_everything(seed=SEED):\n   random.seed(seed)\n   os.environ['PYTHONHASHSEED'] = str(seed)\n   np.random.seed(seed)\n   torch.manual_seed(seed)\n   torch.cuda.manual_seed(seed)\n   torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SPLIT_DIR = '../input/hengs-split'\nDATA_DIR = '../input/severstal-mine'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Logger(object):\n    def __init__(self):\n        self.terminal = sys.stdout  #stdout\n        self.file = None\n\n    def open(self, file, mode=None):\n        if mode is None: mode ='w'\n        self.file = open(file, mode)\n\n    def write(self, message, is_terminal=1, is_file=1 ):\n        if '\\r' in message: is_file=0\n\n        if is_terminal == 1:\n            self.terminal.write(message)\n            self.terminal.flush()\n            #time.sleep(1)\n\n        if is_file == 1:\n            self.file.write(message)\n            self.file.flush()\n\n    def flush(self):\n        # this flush method is needed for python 3 compatibility.\n        # this handles the flush command by doing nothing.\n        # you might want to specify some extra behavior here.\n        pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SteelDataset(Dataset):\n    def __init__(self, split, csv, mode, augment=None):\n\n        self.split   = split\n        self.csv     = csv\n        self.mode    = mode\n        self.augment = augment\n\n        self.uid = list(np.concatenate([np.load(SPLIT_DIR + '/%s'%f , allow_pickle=True) for f in split]))\n        df = pd.concat([pd.read_csv(DATA_DIR + '/%s'%f).fillna('') for f in csv])\n\n        df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n        df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n        df = df_loc_by_list(df, 'ImageId_ClassId', [ u.split('/')[-1] + '_%d'%c  for u in self.uid for c in [1,2,3,4] ])\n        self.df = df\n        self.num_image = len(df)//4\n\n\n    def __str__(self):\n        num1 = (self.df['Class']==1).sum()\n        num2 = (self.df['Class']==2).sum()\n        num3 = (self.df['Class']==3).sum()\n        num4 = (self.df['Class']==4).sum()\n        pos1 = ((self.df['Class']==1) & (self.df['Label']==1)).sum()\n        pos2 = ((self.df['Class']==2) & (self.df['Label']==1)).sum()\n        pos3 = ((self.df['Class']==3) & (self.df['Label']==1)).sum()\n        pos4 = ((self.df['Class']==4) & (self.df['Label']==1)).sum()\n        neg1 = num1-pos1\n        neg2 = num2-pos2\n        neg3 = num3-pos3\n        neg4 = num4-pos4\n\n        length = len(self)\n        num = len(self)\n        pos = (self.df['Label']==1).sum()\n        neg = num-pos\n\n        #---\n\n        string  = ''\n        string += '\\tmode    = %s\\n'%self.mode\n        string += '\\tsplit   = %s\\n'%self.split\n        string += '\\tcsv     = %s\\n'%str(self.csv)\n        string += '\\tnum_image = %8d\\n'%self.num_image\n        string += '\\tlen       = %8d\\n'%len(self)\n        if self.mode == 'train':\n            string += '\\t\\tpos1, neg1 = %5d  %0.3f,  %5d  %0.3f\\n'%(pos1,pos1/num,neg1,neg1/num)\n            string += '\\t\\tpos2, neg2 = %5d  %0.3f,  %5d  %0.3f\\n'%(pos2,pos2/num,neg2,neg2/num)\n            string += '\\t\\tpos3, neg3 = %5d  %0.3f,  %5d  %0.3f\\n'%(pos3,pos3/num,neg3,neg3/num)\n            string += '\\t\\tpos4, neg4 = %5d  %0.3f,  %5d  %0.3f\\n'%(pos4,pos4/num,neg4,neg4/num)\n        return string\n\n\n    def __len__(self):\n        return len(self.uid)\n\n\n    def __getitem__(self, index):\n        # print(index)\n        folder, image_id = self.uid[index].split('/')\n\n        rle = [\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_1','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_2','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_3','EncodedPixels'].values[0],\n            self.df.loc[self.df['ImageId_ClassId']==image_id + '_4','EncodedPixels'].values[0],\n        ]\n        image = cv2.imread(DATA_DIR + '/%s/%s'%(folder,image_id), cv2.IMREAD_COLOR)\n        label = [ 0 if r=='' else 1 for r in rle]\n        mask  = np.array([run_length_decode(r, height=256, width=1600, fill_value=c) for c,r in zip([1,2,3,4],rle)])\n        mask  = mask.max(0, keepdims=0)\n\n        infor = Struct(\n            index    = index,\n            folder   = folder,\n            image_id = image_id,\n        )\n\n        if self.augment is None:\n            return image, label, mask, infor\n        else:\n            return self.augment(image, label, mask, infor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FiveBalanceClassSampler(Sampler):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n        label = (self.dataset.df['Label'].values)\n        label = label.reshape(-1,4)\n        label = np.hstack([label.sum(1,keepdims=True)==0,label]).T\n\n        self.neg_index  = np.where(label[0])[0]\n        self.pos1_index = np.where(label[1])[0]\n        self.pos2_index = np.where(label[2])[0]\n        self.pos3_index = np.where(label[3])[0]\n        self.pos4_index = np.where(label[4])[0]\n\n        #5x\n        self.num_image = len(self.dataset.df)//4\n        self.length = self.num_image*5\n\n\n    def __iter__(self):\n        # neg = self.neg_index.copy()\n        # random.shuffle(neg)\n\n        neg  = np.random.choice(self.neg_index,  self.num_image, replace=True)\n        pos1 = np.random.choice(self.pos1_index, self.num_image, replace=True)\n        pos2 = np.random.choice(self.pos2_index, self.num_image, replace=True)\n        pos3 = np.random.choice(self.pos3_index, self.num_image, replace=True)\n        pos4 = np.random.choice(self.pos4_index, self.num_image, replace=True)\n\n        l = np.stack([neg,pos1,pos2,pos3,pos4]).T\n        l = l.reshape(-1)\n        return iter(l)\n\n    def __len__(self):\n        return self.length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class which is used by the infor object in __get_item__\nclass Struct(object):\n    def __init__(self, is_copy=False, **kwargs):\n        self.add(is_copy, **kwargs)\n\n    def add(self, is_copy=False, **kwargs):\n        #self.__dict__.update(kwargs)\n\n        if is_copy == False:\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        else:\n            for key, value in kwargs.items():\n                try:\n                    setattr(self, key, copy.deepcopy(value))\n                    #setattr(self, key, value.copy())\n                except Exception:\n                    setattr(self, key, value)\n\n    def __str__(self):\n        text =''\n        for k,v in self.__dict__.items():\n            text += '\\t%s : %s\\n'%(k, str(v))\n        return text\n    \n# Creating masks\ndef run_length_decode(rle, height=256, width=1600, fill_value=1):\n    mask = np.zeros((height,width), np.float32)\n    if rle != '':\n        mask=mask.reshape(-1)\n        r = [int(r) for r in rle.split(' ')]\n        r = np.array(r).reshape(-1, 2)\n        for start,length in r:\n            start = start-1  #???? 0 or 1 index ???\n            mask[start:(start + length)] = fill_value\n        mask=mask.reshape(width, height).T\n    return mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def null_collate0(batch):\n    batch_size = len(batch)\n\n    input = []\n    truth_label = []\n    truth_mask  = []\n    infor = []\n    for b in range(batch_size):\n        input.append(batch[b][0])\n        truth_label.append(batch[b][1])\n        truth_mask.append(batch[b][2])\n        infor.append(batch[b][3])\n\n    input = np.stack(input).astype(np.float32)/255\n    input = input.transpose(0,3,1,2)\n    truth_label = np.stack(truth_label)\n    truth_mask  = np.stack(truth_mask)\n\n    input = torch.from_numpy(input).float()\n    truth_label = torch.from_numpy(truth_label).float()\n    truth_mask = torch.from_numpy(truth_mask).long().unsqueeze(1)\n\n    return input, truth_label, truth_mask, infor\n\ndef null_collate(batch):\n    input, truth_label, truth_mask, infor = null_collate0(batch)\n    with torch.no_grad():\n        arange = torch.FloatTensor([1,2,3,4]).to(truth_mask.device).view(1,4,1,1).long()\n        m = truth_mask.repeat(1,4,1,1)\n        m = (m==arange).float()\n        truth_attention = F.avg_pool2d(m,kernel_size=(32,32),stride=(32,32))\n        truth_attention = (truth_attention > 0/(32*32)).float()\n\n        #relabel for augmentation cropping, etc\n        truth_label = m.sum(dim=[2,3])\n        truth_label = (truth_label > 1).float()\n\n    return input, truth_label, truth_mask, truth_attention, infor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_augment1(image, label, mask, infor):\n    u=np.random.choice(3)\n    if   u==0:\n        pass\n    elif u==1:\n        image, mask = do_random_crop_rescale(image,mask,1600-(256-180),180)\n    elif u==2:\n        image, mask = do_random_crop_rotate_rescale(image,mask,1600-(256-200),200)\n\n    #---------\n    image, mask = do_random_crop(image, mask, 400,256)\n\n    if np.random.rand()>0.25:\n         image, mask = do_random_cutout(image, mask)\n\n\n    #---------\n    if np.random.rand()>0.5:\n        image, mask = do_flip_lr(image, mask)\n    if np.random.rand()>0.5:\n        image, mask = do_flip_ud(image, mask)\n\n    #---------\n    if np.random.rand()>0.5:\n        image = do_random_log_contast(image, gain=[0.50, 1.75])\n\n    #---------\n    u=np.random.choice(2)\n    if   u==0:\n        pass\n    if   u==1:\n        image = do_random_noise(image, noise=8)\n#     if   u==2:\n#         image = do_random_salt_pepper_noise(image, noise =0.0001)\n    # if   u==3:\n    #     image = do_random_salt_pepper_line(image)\n\n    return image, label, mask, infor\n\n# Learning Rate Schedule\nclass NullScheduler():\n    def __init__(self, lr=0.01 ):\n        super(NullScheduler, self).__init__()\n        self.lr    = lr\n        self.cycle = 0\n\n    def __call__(self, time):\n        return self.lr\n\n    def __str__(self):\n        string = 'NullScheduler\\n' \\\n                + 'lr=%0.5f '%(self.lr)\n        return string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BatchNorm2d = nn.BatchNorm2d\n\nPRETRAIN_FILE = '../input/efficientnet-pytorch-b0-b7/efficientnet-b5-b6417697.pth'\ndef load_pretrain(net, skip=[], pretrain_file=PRETRAIN_FILE, conversion=CONVERSION, is_print=True):\n\n    #raise NotImplementedError\n    print('\\tload pretrain_file: %s'%pretrain_file)\n\n    #pretrain_state_dict = torch.load(pretrain_file)\n    pretrain_state_dict = torch.load(pretrain_file, map_location=lambda storage, loc: storage)\n    state_dict = net.state_dict()\n\n    i = 0\n    conversion = np.array(CONVERSION).reshape(-1,4)\n    for key,_,pretrain_key,_ in conversion:\n        if any(s in key for s in\n            ['.num_batches_tracked',]+skip):\n            continue\n\n        #print('\\t\\t',key)\n        if is_print:\n            print('\\t\\t','%-48s  %-24s  <---  %-32s  %-24s'%(\n                key, str(state_dict[key].shape),\n                pretrain_key, str(pretrain_state_dict[pretrain_key].shape),\n            ))\n        i = i+1\n\n        state_dict[key] = pretrain_state_dict[pretrain_key]\n\n    net.load_state_dict(state_dict)\n    print('')\n    print('len(pretrain_state_dict.keys()) = %d'%len(pretrain_state_dict.keys()))\n    print('len(state_dict.keys())          = %d'%len(state_dict.keys()))\n    print('loaded    = %d'%i)\n    print('')\n\nclass ConvGnUp2d(nn.Module):\n    def __init__(self, in_channel, out_channel, num_group=32, kernel_size=3, padding=1, stride=1):\n        super(ConvGnUp2d, self).__init__()\n        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n        self.gn   = nn.GroupNorm(num_group,out_channel)\n\n    def forward(self,x):\n        x = self.conv(x)\n        x = self.gn(x)\n        x = F.relu(x, inplace=True)\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        return x\n\n\ndef upsize_add(x, lateral):\n    return F.interpolate(x, size=lateral.shape[2:], mode='nearest') + lateral\n\ndef upsize(x, scale_factor=2):\n    x = F.interpolate(x, scale_factor=scale_factor, mode='nearest')\n    return x\n\n'''\nmodel.py: calling main function ... \n \n\nstem   torch.Size([10, 48, 128, 128])\nblock1 torch.Size([10, 24, 128, 128])\n\nblock2 torch.Size([10, 40, 64, 64])\n\nblock3 torch.Size([10, 64, 32, 32])\n\nblock4 torch.Size([10, 128, 16, 16])\nblock5 torch.Size([10, 176, 16, 16])\n\nblock6 torch.Size([10, 304, 8, 8])\nblock7 torch.Size([10, 512, 8, 8])\nlast   torch.Size([10, 2048, 8, 8])\n\nsucess!\n'''\n\n\nclass Net(nn.Module):\n    def load_pretrain(self, skip=['logit.'], is_print=True):\n        load_pretrain(self, skip, pretrain_file=PRETRAIN_FILE, conversion=CONVERSION, is_print=is_print)\n\n\n\n    def __init__(self, num_class=4, drop_connect_rate=0.2):\n        super(Net, self).__init__()\n\n        e = EfficientNetB5(drop_connect_rate)\n        self.stem   = e.stem\n        self.block1 = e.block1\n        self.block2 = e.block2\n        self.block3 = e.block3\n        self.block4 = e.block4\n        self.block5 = e.block5\n        self.block6 = e.block6\n        self.block7 = e.block7\n        self.last   = e.last\n        e = None  #dropped\n\n        #---\n        self.lateral0 = nn.Conv2d(2048, 64,  kernel_size=1, padding=0, stride=1)\n        self.lateral1 = nn.Conv2d( 176, 64,  kernel_size=1, padding=0, stride=1)\n        self.lateral2 = nn.Conv2d(  64, 64,  kernel_size=1, padding=0, stride=1)\n        self.lateral3 = nn.Conv2d(  40, 64,  kernel_size=1, padding=0, stride=1)\n\n        self.top1 = nn.Sequential(\n            ConvGnUp2d( 64, 64),\n            ConvGnUp2d( 64, 64),\n            ConvGnUp2d( 64, 64),\n        )\n        self.top2 = nn.Sequential(\n            ConvGnUp2d( 64, 64),\n            ConvGnUp2d( 64, 64),\n        )\n        self.top3 = nn.Sequential(\n            ConvGnUp2d( 64, 64),\n        )\n        self.top4 = nn.Sequential(\n            nn.Conv2d(64*3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n            BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n        self.logit_mask = nn.Conv2d(64,num_class+1,kernel_size=1)\n\n\n\n\n\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n\n        x = self.stem(x)            #; print('stem  ',x.shape)\n        x = self.block1(x)    ;x0=x #; print('block1',x.shape)\n        x = self.block2(x)    ;x1=x #; print('block2',x.shape)\n        x = self.block3(x)    ;x2=x #; print('block3',x.shape)\n        x = self.block4(x)          #; print('block4',x.shape)\n        x = self.block5(x)    ;x3=x #; print('block5',x.shape)\n        x = self.block6(x)          #; print('block6',x.shape)\n        x = self.block7(x)          #; print('block7',x.shape)\n        x = self.last(x)      ;x4=x #; print('last  ',x.shape)\n\n        # segment\n        t0 = self.lateral0(x4)\n        t1 = upsize_add(t0, self.lateral1(x3)) #16x16\n        t2 = upsize_add(t1, self.lateral2(x2)) #32x32\n        t3 = upsize_add(t2, self.lateral3(x1)) #64x64\n\n        t1 = self.top1(t1) #128x128\n        t2 = self.top2(t2) #128x128\n        t3 = self.top3(t3) #128x128\n\n        t = torch.cat([t1,t2,t3],1)\n        t = self.top4(t)\n        logit_mask = self.logit_mask(t)\n        logit_mask = F.interpolate(logit_mask, scale_factor=2.0, mode='bilinear', align_corners=False)\n\n        return logit_mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# METRICS\n# use topk\n# def criterion_label(logit, truth, weight=None):\n#     batch_size,num_class,H,W = logit.shape\n#     K=5\n#\n#     logit = logit.view(batch_size,num_class,-1)\n#     value, index = logit.topk(K)\n#\n#     logit_k = torch.gather(logit,dim=2,index=index)\n#     truth_k = truth.view(batch_size,num_class,1).repeat(1,1,5)\n#\n#\n#     if weight is None: weight=[1,1,1,1]\n#     weight = torch.FloatTensor(weight).to(truth.device).view(1,-1,1)\n#\n#\n#     loss = F.binary_cross_entropy_with_logits(logit_k, truth_k, reduction='none')\n#     #https://arxiv.org/pdf/1909.07829.pdf\n#     if 1:\n#         gamma=2.0\n#         p = torch.sigmoid(logit_k)\n#         focal = (truth_k*(1-p) + (1-truth_k)*(p))**gamma\n#         weight = weight*focal /focal.sum().item()\n#\n#     loss = loss*weight\n#     loss = loss.mean()\n#     return loss\n\n\n#use top only\n# def criterion_label(logit, truth, weight=None):\n#     batch_size,num_class,H,W = logit.shape\n#     logit = F.adaptive_max_pool2d(logit,1).view(-1,4)\n#     truth = truth.view(-1,4)\n#\n#     if weight is None: weight=[1,1,1,1]\n#     weight = torch.FloatTensor(weight).to(truth.device).view(1,-1)\n#\n#     loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n#     loss = loss*weight\n#     loss = loss.mean()\n#     return loss\n\n\n\n\n#https://discuss.pytorch.org/t/numerical-stability-of-bcewithlogitsloss/8246\ndef criterion_attention(logit, truth, weight=None):\n    batch_size,num_class, H,W = logit.shape\n\n    if weight is None: weight=[1,1,1,1]\n    weight = torch.FloatTensor(weight).to(truth.device).view(1,-1,1,1)\n\n    loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n\n    #---\n    #https://arxiv.org/pdf/1909.07829.pdf\n    if 0:\n        gamma=2.0\n        p = torch.sigmoid(logit)\n        focal = (truth*(1-p) + (1-truth)*(p))**gamma\n        weight = weight*focal /focal.sum().item()*H*W\n    #---\n    loss = loss*weight\n    loss = loss.mean()\n    return loss\n\n#\n# def criterion_mask(logit, truth, weight=None):\n#     if weight is not None: weight = torch.FloatTensor([1]+weight).cuda()\n#     batch_size,num_class,H,W = logit.shape\n#\n#     logit = logit.permute(0, 2, 3, 1).contiguous().view(batch_size,-1, 5)\n#     log_probability = -F.log_softmax(logit,-1)\n#\n#\n#     truth = truth.permute(0, 2, 3, 1).contiguous().view(-1,1)\n#     onehot = torch.FloatTensor(batch_size*H*W, 5).to(truth.device)\n#     onehot.zero_()\n#     onehot.scatter_(1, truth, 1)\n#     onehot = onehot.view(batch_size,-1, 5)\n#\n#     #loss = F.cross_entropy(logit, truth, weight=weight, reduction='none')\n#     loss = log_probability*onehot\n#\n#     loss = loss*weight\n#     loss = loss.mean()\n#     return loss\n\n#focal loss\ndef criterion_mask(logit, truth, weight=None):\n    if weight is None: weight=[1,1,1,1]\n    weight = torch.FloatTensor([1]+weight).to(truth.device).view(1,-1 )\n\n    batch_size,num_class,H,W = logit.shape\n\n    logit = logit.permute(0, 2, 3, 1).contiguous().view(-1, 5)\n    truth = truth.permute(0, 2, 3, 1).contiguous().view(-1)\n    # return F.cross_entropy(logit, truth, reduction='mean')\n\n    log_probability = -F.log_softmax(logit,-1)\n    probability = F.softmax(logit,-1)\n\n    onehot = torch.zeros(batch_size*H*W,num_class).to(truth.device)\n    onehot.scatter_(dim=1, index=truth.view(-1,1),value=1) #F.one_hot(truth,5).float()\n\n    loss = log_probability*onehot\n\n    #---\n    if 1:#image based focusing\n        probability = probability.view(batch_size,H*W,5)\n        truth  = truth.view(batch_size,H*W,1)\n        weight = weight.view(1,1,5)\n\n        alpha  = 2\n        focal  = torch.gather(probability, dim=-1, index=truth.view(batch_size,H*W,1))\n        focal  = (1-focal)**alpha\n        focal_sum = focal.sum(dim=[1,2],keepdim=True)\n        #focal_sum = focal.sum().view(1,1,1)\n        weight = weight*focal/focal_sum.detach() *H*W\n        weight = weight.view(-1,5)\n\n    #---\n    if 0:#add topk max pool loss\n        #https://discuss.pytorch.org/t/resolved-how-to-implement-k-max-pooling-for-cnn-text-classification/931\n        #probability = probability.view(batch_size,H*W,5)\n        #weight = weight.view(1,1,5)\n        with torch.no_grad():\n            index = probability.topk(k=5, dim = 1)[1].sort(dim = 1)[0]\n            topk = torch.ones(batch_size*H*W,num_class).to(truth.device)\n            topk[index] = 2.0  #increase weighing\n            topk = topk.view(-1,5)\n        weight = weight*topk\n\n        zz=0\n\n\n    loss = loss*weight\n    loss = loss.mean()\n    return loss\n\n#----\ndef logit_mask_to_probability_label(logit):\n    batch_size,num_class,H,W = logit.shape\n    probability = F.softmax(logit,1)\n    #probability = F.avg_pool2d(probability, kernel_size=16,stride=16)\n\n    probability = probability.permute(0, 2, 3, 1).contiguous().view(batch_size,-1, 5)\n    value,index = probability.max(1)\n\n    probability = value[:,1:]\n    return probability\n\ndef metric_label(probability, truth, threshold=0.5):\n    batch_size=len(truth)\n\n    with torch.no_grad():\n        probability = probability.view(batch_size,4)\n        truth = truth.view(batch_size,4)\n\n        #----\n        neg_index = (truth==0).float()\n        pos_index = 1-neg_index\n        num_neg = neg_index.sum(0)\n        num_pos = pos_index.sum(0)\n\n        #----\n        p = (probability>threshold).float()\n        t = (truth>0.5).float()\n\n        tp = ((p + t) == 2).float()  # True positives\n        tn = ((p + t) == 0).float()  # True negatives\n        tn = tn.sum(0)\n        tp = tp.sum(0)\n\n        #----\n        tn = tn.data.cpu().numpy()\n        tp = tp.data.cpu().numpy()\n        num_neg = num_neg.data.cpu().numpy().astype(np.int32)\n        num_pos = num_pos.data.cpu().numpy().astype(np.int32)\n\n    return tn,tp, num_neg,num_pos\n\ndef truth_to_onehot(truth, num_class=4):\n    onehot = truth.repeat(1,num_class,1,1)\n    arange = torch.arange(1,num_class+1).view(1,num_class,1,1).to(truth.device)\n    onehot = (onehot == arange).float()\n    return onehot\n\ndef predict_to_onehot(predict, num_class=4):\n    value, index = torch.max(predict, 1, keepdim=True)\n    value  = value.repeat(1,num_class,1,1)\n    index  = index.repeat(1,num_class,1,1)\n    arange = torch.arange(1,num_class+1).view(1,num_class,1,1).to(predict.device)\n    onehot = (index == arange).float()\n    value  = value*onehot\n    return value\n\ndef metric_mask(logit, truth, threshold=0.5, sum_threshold=100):\n    with torch.no_grad():\n        probability = torch.softmax(logit,1)\n        truth = truth_to_onehot(truth)\n        probability = predict_to_onehot(probability)\n\n        batch_size,num_class,H,W = truth.shape\n        probability = probability.view(batch_size,num_class,-1)\n        truth = truth.view(batch_size,num_class,-1)\n        p = (probability>threshold).float()\n        t = (truth>0.5).float()\n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n        d_neg = (p_sum < sum_threshold).float()\n        d_pos = 2*(p*t).sum(-1)/((p+t).sum(-1)+1e-12)\n\n        neg_index = (t_sum==0).float()\n        pos_index = 1-neg_index\n\n        num_neg = neg_index.sum(0)\n        num_pos = pos_index.sum(0)\n        dn = (neg_index*d_neg).sum(0)\n        dp = (pos_index*d_pos).sum(0)\n\n        #----\n        dn = dn.data.cpu().numpy()\n        dp = dp.data.cpu().numpy()\n        num_neg = num_neg.data.cpu().numpy().astype(np.int32)\n        num_pos = num_pos.data.cpu().numpy().astype(np.int32)\n\n    return dn,dp, num_neg,num_pos\n\ndef probability_mask_to_probability_label(probability):\n    batch_size,num_class,H,W = probability.shape\n    probability = probability.permute(0, 2, 3, 1).contiguous().view(batch_size,-1, 5)\n    value, index = probability.max(1)\n    probability = value[:,1:]\n    return probability","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef get_learning_rate(optimizer):\n    lr=[]\n    for param_group in optimizer.param_groups:\n       lr +=[ param_group['lr'] ]\n\n    assert(len(lr)==1) #we support only one param_group\n    lr = lr[0]\n\n    return lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_valid(net, valid_loader, out_dir=None):\n\n    valid_loss = np.zeros(17, np.float32)\n    valid_num  = np.zeros_like(valid_loss)\n\n    for t, (input, truth_label, truth_mask, truth_attention, infor) in enumerate(valid_loader):\n\n        #if b==5: break\n        batch_size = len(infor)\n\n        net.eval()\n        input = input.cuda()\n        truth_label = truth_label.cuda()\n        truth_mask  = truth_mask.cuda()\n        truth_attention = truth_attention.cuda()\n\n        with torch.no_grad():\n            logit_mask = net(input)\n            loss = criterion_mask(logit_mask, truth_mask)\n\n            probability_mask  = F.softmax(logit_mask,1)\n            probability_label = probability_mask_to_probability_label(probability_mask)\n            tn,tp, num_neg,num_pos = metric_label(probability_label, truth_label)\n            dn,dp, num_neg,num_pos = metric_mask(logit_mask, truth_mask)\n\n        #---\n        l = np.array([ loss.item()*batch_size, *tn, *tp, *dn, *dp])\n        n = np.array([ batch_size, *num_neg, *num_pos, *num_neg, *num_pos])\n        valid_loss += l\n        valid_num  += n\n\n        #==========\n        #dum results for debug\n#         if 0:\n\n#             probability_mask  = F.softmax(logit_mask,1)\n\n#             probability_label = probability_label.data.cpu().numpy()\n#             probability_mask = probability_mask.data.cpu().numpy()\n#             truth_label = truth_label.data.cpu().numpy()\n#             truth_mask  = truth_mask.data.cpu().numpy()\n\n\n#             image = input_to_image(input)\n#             for b in range(batch_size):\n#                 image_id = infor[b].image_id\n#                 result = draw_predict_result(\n#                     image[b], truth_label[b], truth_mask[b], probability_label[b], probability_mask[b])\n\n#                 image_show('result',result,resize=0.5)\n#                 cv2.imwrite(out_dir +'/valid/%s.png'%image_id[:-4], result)\n#                 cv2.waitKey(1)\n#                 pass\n#         #==========\n\n        #print(valid_loss)\n        print('\\r %4d/%4d'%(valid_num[0], len(valid_loader.dataset)),end='',flush=True)\n\n        pass  #-- end of one data loader --\n    assert(valid_num[0] == len(valid_loader.dataset))\n    valid_loss = valid_loss/valid_num\n\n    return valid_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_train():\n    out_dir = \\\n        ''\n    initial_checkpoint = \\\n        '../input/effb5-mishfpn/00045000_model.pth'\n    \n    sampler     = FiveBalanceClassSampler #RandomSampler #FiveBalanceClassSampler\n    loss_weight = None #[5,10,2,5]\n\n    schduler = NullScheduler(lr=0.001)\n    iter_accum = 1\n    batch_size =4 #8\n    \n    log = Logger()\n    log.open('../working/log.train.txt',mode='a')\n    log.write('\\n')\n    log.write('\\tSEED         = %u\\n' % SEED)\n    \n    log.write('** dataset setting **\\n')\n    train_dataset = SteelDataset(\n        mode    = 'train',\n        csv     = ['train.csv',],\n        split   = ['train_b1_11568.npy',],\n        augment = train_augment1,\n    )\n    train_loader  = DataLoader(\n        train_dataset,\n        sampler     = sampler(train_dataset),\n        batch_size  = batch_size,\n        drop_last   = True,\n        num_workers = 4,\n        pin_memory  = True,\n        collate_fn  = null_collate\n    )\n    \n    valid_dataset = SteelDataset(\n        mode    = 'train',\n        csv     = ['train.csv',],\n        split   = ['valid_b1_1000.npy',],\n        augment = None,\n    )\n    valid_loader = DataLoader(\n        valid_dataset,\n        sampler     = SequentialSampler(valid_dataset),\n        batch_size  = 4,\n        drop_last   = False,\n        num_workers = 4,\n        pin_memory  = True,\n        collate_fn  = null_collate\n    )\n\n    assert(len(train_dataset)>=batch_size)\n    log.write('batch_size = %d\\n'%(batch_size))\n    log.write('train_dataset : \\n%s\\n'%(train_dataset))\n    log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n    log.write('\\n')\n    \n    log.write('** net setting **\\n')\n    net = Net().cuda()\n#     log.write('\\tinitial_checkpoint = %s\\n' % initial_checkpoint)\n    if initial_checkpoint is not None:\n        state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n        # for k in list(state_dict.keys()):\n        #     if any(s in k for s in ['g_block1',]): state_dict.pop(k, None)\n        # net.load_state_dict(state_dict,strict=False)\n        net.load_state_dict(state_dict,strict=False)  #True\n    else:\n        net.load_pretrain(is_print=False)\n    \n    log.write('%s\\n'%(type(net)))\n    log.write('loss_weight=%s\\n'%(str(loss_weight)))\n    log.write('sampler=%s\\n'%(str(sampler)))\n    log.write('\\n')\n    \n    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=schduler(0), momentum=0.9, weight_decay=0.0001)\n\n    num_iters   = 3000*1000\n    iter_smooth = 50\n    iter_log    = 200\n    iter_valid  = 200\n    iter_save   = [0, num_iters-1]\\\n                   + list(range(0, num_iters, 2500))#1*1000\n\n    start_iter = 0\n    start_epoch= 0\n    rate       = 0\n    if initial_checkpoint is not None:\n        initial_optimizer = initial_checkpoint.replace('_model.pth','_optimizer.pth')\n        if os.path.exists(initial_optimizer):\n            checkpoint  = torch.load(initial_optimizer)\n            start_iter  = checkpoint['iter' ]\n            start_epoch = checkpoint['epoch']\n            #optimizer.load_state_dict(checkpoint['optimizer'])\n        pass\n\n    log.write('optimizer\\n  %s\\n'%(optimizer))\n    log.write('schduler\\n  %s\\n'%(schduler))\n    log.write('\\n')\n    \n    log.write('** start training here! **\\n')\n    log.write('   batch_size=%d,  iter_accum=%d\\n'%(batch_size,iter_accum))\n    log.write('                     |------------------------------------------- VALID------------------------------------------------|---------------------- TRAIN/BATCH ---------------------\\n')\n    log.write('rate     iter  epoch |  loss           [tn1,2,3,4  :  tp1,2,3,4]                    [dn1,2,3,4  :  dp1,2,3,4]          |  loss    [tn :  tp1,2,3,4]          | time             \\n')\n    log.write('--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n')\n              #0.00000 135.0*  65.1 |   nan  [1.00 1.00 1.00 1.00 : 0.00 0.00 0.00 0.00]  [1.00 1.00 1.00 1.00 : 0.00 0.00 0.00 0.00] | 0.000  [0.00 : 0.00 0.00 0.00 0.00] |  0 hr 00 min\n\n    valid_loss = np.zeros(17,np.float32)\n    train_loss = np.zeros( 6,np.float32)\n    batch_loss = np.zeros_like(valid_loss)\n    iter = 0\n    i    = 0\n    \n    start = timer()\n    while  iter<num_iters:\n        sum_train_loss = np.zeros_like(train_loss)\n        sum_train = np.zeros_like(train_loss)\n\n        optimizer.zero_grad()\n        for t, (input, truth_label, truth_mask, truth_attention, infor) in enumerate(train_loader):\n\n            batch_size = len(infor)\n            iter  = i + start_iter\n            epoch = (iter-start_iter)*batch_size/len(train_dataset) + start_epoch\n\n\n            #if 0:\n            if (iter % iter_valid==0):\n                valid_loss = do_valid(net, valid_loader, out_dir) #\n                pass\n\n            if (iter % iter_log==0):\n                print('\\r',end='',flush=True)\n                asterisk = '*' if iter in iter_save else ' '\n                log.write('%0.5f %5.1f%s %5.1f | %5.3f  [%0.2f %0.2f %0.2f %0.2f : %0.2f %0.2f %0.2f %0.2f]  [%0.2f %0.2f %0.2f %0.2f : %0.2f %0.2f %0.2f %0.2f] | %5.3f  [%0.2f : %0.2f %0.2f %0.2f %0.2f] | %s' % (\\\n                         rate, iter/1000, asterisk, epoch,\n                         *valid_loss,\n                         *train_loss,\n                         time_to_str((timer() - start),'min'))\n                )\n                log.write('\\n')\n                \n            if iter in iter_save:\n                torch.save({\n                    #'optimizer': optimizer.state_dict(),\n                    'iter'     : iter,\n                    'epoch'    : epoch,\n                }, '../working/%08d_optimizer.pth'%(iter))\n                if iter!=start_iter:\n                    torch.save(net.state_dict(),'../working/%08d_model.pth'%(iter))\n                    pass\n\n\n            # learning rate schduler -------------\n            lr = schduler(iter)\n            if lr<0 : break\n            adjust_learning_rate(optimizer, lr)\n            rate = get_learning_rate(optimizer)\n\n            # one iteration update  -------------\n            #net.set_mode('train',is_freeze_bn=True)\n\n            net.train()\n            input = input.cuda()\n            truth_label = truth_label.cuda()\n            truth_mask  = truth_mask.cuda()\n#             truth_attention  = truth_attention.cuda()\n\n\n            logit_mask = net(input)\n            loss = criterion_mask(logit_mask, truth_mask)\n            probability_mask  = F.softmax(logit_mask,1)\n            probability_label = probability_mask_to_probability_label(probability_mask)\n            tn,tp, num_neg,num_pos = metric_label(probability_label, truth_label)\n            \n            ((loss)/iter_accum).backward()\n            if (iter % iter_accum)==0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # print statistics  --------\n            l = np.array([ loss.item()*batch_size,tn.sum(),*tp ])\n            n = np.array([ batch_size, num_neg.sum(),*num_pos ])\n            batch_loss      = l/(n+1e-8)\n            sum_train_loss += l\n            sum_train      += n\n            if iter%iter_smooth == 0:\n                train_loss = sum_train_loss/(sum_train+1e-12)\n                sum_train_loss[...] = 0\n                sum_train[...]      = 0\n\n\n            print('\\r',end='',flush=True)\n            asterisk = ' '\n            print('%0.5f %5.1f%s %5.1f | %5.3f  [%0.2f %0.2f %0.2f %0.2f : %0.2f %0.2f %0.2f %0.2f]  [%0.2f %0.2f %0.2f %0.2f : %0.2f %0.2f %0.2f %0.2f] | %5.3f  [%0.2f : %0.2f %0.2f %0.2f %0.2f] | %s' % (\\\n                         rate, iter/1000, asterisk, epoch,\n                         *valid_loss,\n                         *batch_loss,\n                         time_to_str((timer() - start),'min'))\n            , end='',flush=True)\n            i=i+1\n            \n            if 0:\n                for di in range(3):\n                    if (iter+di)%1000==0:\n\n                        probability_attention = torch.sigmoid(logit_attention)\n                        probability_attention = probability_attention.data.cpu().numpy().reshape(-1,4,5)\n                        truth_label = truth_label.data.cpu().numpy()\n                        truth_mask  = truth_mask.data.cpu().numpy()\n                        truth_attention = truth_attention.data.cpu().numpy().reshape(-1,4,5)\n\n                        image = input_to_image(input)\n                        for b in range(batch_size):\n                            image_id = infor[b].image_id\n                            result = draw_predict_result_8cls(image[b], truth_label[b], truth_mask[b], truth_attention[b], probability_attention[b])\n\n                            image_show('result',result,resize=0.5)\n                            cv2.imwrite(out_dir +'/train/%05d.png'%(di*100+b), result)\n                            cv2.waitKey(1)\n                            pass\n\n\n\n\n        pass  #-- end of one data loader --\n    pass #-- end of all iterations --\n\n    log.write('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('                     |------------------------------------------- VALID------------------------------------------------|---------------------- TRAIN/BATCH ---------------------\\n')\nprint('rate     iter  epoch |  loss           [tn1,2,3,4  :  tp1,2,3,4]                    [dn1,2,3,4  :  dp1,2,3,4]          |  loss    [tn :  tp1,2,3,4]          | time             \\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_train()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}